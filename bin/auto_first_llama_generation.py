from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate
from langchain_groq import ChatGroq

from langchain_core.pydantic_v1 import BaseModel, Field
import time
import json
import os
import pandas as pd
import logging
from tqdm import tqdm
import traceback

from dotenv import load_dotenv
load_dotenv()

logger = logging.getLogger(__name__)
logging.basicConfig(filename='first_llama_generation.log', level=logging.INFO)


class DescriptionObject(BaseModel):
    description: str = Field(description="summarized description")


llm = ChatGroq(model="llama3-70b-8192", temperature=0.5)

prompt_template_llama2_general = """
You are an advanced language model tasked with merging detailed captions generated by two multimodal systems. Your goal is to create a unified, highly detailed, and factually accurate description of the image.

### Instructions:
- Combine the details from both captions, ensuring all critical elements (objects, actions, spatial relationships, and contextual nuances) are represented.
- Maintain a coherent and consistent tone throughout the output.
- Avoid contradictions or redundant details; instead, refine and enhance the combined information for clarity.
- Incorporate additional entities provided, enriching the description with fine-grained visual and contextual details.
- The resulting caption should serve as the most comprehensive and accurate summary of the image.
"""

system_message = SystemMessagePromptTemplate.from_template(prompt_template_llama2_general)
human_message = HumanMessagePromptTemplate.from_template("""
### Input Captions:
1. Caption 1: {caption1}
2. Caption 2: {caption2}

### Additional Entities:
- {entities}""")

prompt_object_detection = ChatPromptTemplate.from_messages([system_message, human_message])
model = prompt_object_detection | llm.with_structured_output(DescriptionObject)

request_counter = 0
last_reset_time = time.time()


def invoke_with_rate_limit_per_minute(caption1, caption2, entities, requests_per_minute=29):
    global request_counter, last_reset_time

    current_time = time.time()
    if current_time - last_reset_time >= 60:
        request_counter = 0
        last_reset_time = current_time

    if request_counter >= requests_per_minute:
        wait_time = 60 - (current_time - last_reset_time)
        print(f"Rate limit reached. Please wait {wait_time:.2f} seconds.")
        time.sleep(wait_time)  # Sleep until the next allowed time
        request_counter = 0  # Reset the counter after waiting
        last_reset_time = time.time()

    # Now make the request
    response = model.invoke({"caption1": caption1, "caption2": caption2, "entities": entities}).description

    # Increment the request counter after the request
    request_counter += 1

    return response


root = r"C:\Users\Gram\Desktop\NULP\uav_img_cap"

with open(os.path.join(root, "processed_data/inference_results_llava_first_step.json"), "r") as f:
    inference_llava = json.loads(f.read())

with open(os.path.join(root, "processed_data/inference_results_kosmos_2.json"), "r") as f:
    inference_kosmos = json.loads(f.read())

merged_df = pd.DataFrame(inference_llava).merge(pd.DataFrame(inference_kosmos), on="image", suffixes=("_llava", "_kosmos"))


with open(os.path.join(root, "bin/inference_llama_first_80b.json"), "r") as f:
        first_llama_inference = json.loads(f.read())

checked_images = pd.DataFrame(first_llama_inference).image.unique().tolist()
logger.info(f"amount of generated descriptions {len(checked_images)}")

logger.info("started generating descriptions")
for i, r in tqdm(merged_df[~merged_df.image.isin(checked_images)].iterrows(),
                 total=(~merged_df.image.isin(checked_images)).sum()):
    try:
        entities = list(map(lambda entity: entity[0], r["entities"]))
        first_llama_inference.append({"image": r["image"],
                                      "description": invoke_with_rate_limit_per_minute(r["description_llava"],
                                                                                       r["description_kosmos"],
                                                                                       entities)})
    except Exception as e:
        logger.info(f"got exception {traceback.format_exc()}")
        raise e

    finally:
        with open("inference_llama_first_80b.json", "w") as f:
            json.dump(first_llama_inference, f, indent=4)
        logger.info(f"finished with {len(first_llama_inference)} data points processed")
