{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60bd5ac0-7a6d-4ada-97fb-97da2a619f50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MAR20 imports\n",
    "import os\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from PIL import Image\n",
    "# import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a49e74-fb73-422f-ba45-dd03b85a512d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# MAR20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b576b3e6-86ca-4d79-a2ec-81564fd39bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'path_to_mar20_dataset'\n",
    "images_dir = os.path.join(dataset_path, 'images')\n",
    "annotations_dir = os.path.join(dataset_path, 'annotations')\n",
    "\n",
    "# Load the annotations\n",
    "def load_annotations(annotation_file):\n",
    "    with open(annotation_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    annotations = []\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        class_id = int(parts[0])\n",
    "        x_center, y_center, width, height = map(float, parts[1:5])\n",
    "        annotations.append({\n",
    "            'class_id': class_id,\n",
    "            'x_center': x_center,\n",
    "            'y_center': y_center,\n",
    "            'width': width,\n",
    "            'height': height\n",
    "        })\n",
    "    return annotations\n",
    "\n",
    "# Preprocess the images and annotations\n",
    "def preprocess_image(image_path, annotations, target_size=(224, 224)):\n",
    "    # Load the image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    original_width, original_height = image.size\n",
    "    \n",
    "    # Resize the image\n",
    "    image = image.resize(target_size)\n",
    "    resized_width, resized_height = target_size\n",
    "    \n",
    "    # Adjust the annotations for the resized image\n",
    "    resized_annotations = []\n",
    "    for ann in annotations:\n",
    "        x_center = ann['x_center'] * resized_width / original_width\n",
    "        y_center = ann['y_center'] * resized_height / original_height\n",
    "        width = ann['width'] * resized_width / original_width\n",
    "        height = ann['height'] * resized_height / original_height\n",
    "        resized_annotations.append({\n",
    "            'class_id': ann['class_id'],\n",
    "            'x_center': x_center,\n",
    "            'y_center': y_center,\n",
    "            'width': width,\n",
    "            'height': height\n",
    "        })\n",
    "    \n",
    "    # Convert image to numpy array\n",
    "    image_array = np.array(image)\n",
    "    \n",
    "    return image_array, resized_annotations\n",
    "\n",
    "# Example usage\n",
    "image_filename = 'example_image.jpg'  # Replace with an actual image filename\n",
    "annotation_filename = 'example_annotation.txt'  # Replace with an actual annotation filename\n",
    "\n",
    "image_path = os.path.join(images_dir, image_filename)\n",
    "annotation_path = os.path.join(annotations_dir, annotation_filename)\n",
    "\n",
    "annotations = load_annotations(annotation_path)\n",
    "image_array, resized_annotations = preprocess_image(image_path, annotations)\n",
    "\n",
    "# Display the image with annotations\n",
    "def display_image_with_annotations(image, annotations):\n",
    "    plt.imshow(image)\n",
    "    for ann in annotations:\n",
    "        x_center, y_center, width, height = ann['x_center'], ann['y_center'], ann['width'], ann['height']\n",
    "        x_min = int(x_center - width / 2)\n",
    "        y_min = int(y_center - height / 2)\n",
    "        x_max = int(x_center + width / 2)\n",
    "        y_max = int(y_center + height / 2)\n",
    "        cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (255, 0, 0), 2)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "display_image_with_annotations(image_array, resized_annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe633b5c-22e2-43b1-816d-6920d70bc63a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Russian-military-annotated by\n",
    "- Tuomo Hiippala\n",
    "- Digital Geography Lab\n",
    "- Department of Geosciences and Geography\n",
    "- University of Helsinki, Finland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6cf8b52-1ddf-4729-b795-bcb271832db1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in Russian-military-annotated-4 to tfrecord:: 100%|████████████████████████████████████████████████████████████| 33836/33836 [00:23<00:00, 1419.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to Russian-military-annotated-4 in tfrecord:: 100%|████████████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 177.03it/s]\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from roboflow import Roboflow\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "rf = Roboflow(api_key=os.getenv(\"ROBFLOW_API_KEY\"))\n",
    "project = rf.workspace(\"capstoneproject\").project(\"russian-military-annotated\")\n",
    "version = project.version(4)\n",
    "dataset = version.download(\"tfrecord\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02e95b33-6329-414f-9cda-e8442c222f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Gram\\\\Desktop\\\\NULP\\\\uav_img_cap\\\\notebooks\\\\Russian-military-annotated-4'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49d21cc8-6ae1-43c0-8f78-667c91f57b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gram\\Desktop\\NULP\\uav_img_cap\\diploma_uni_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\Gram\\Desktop\\NULP\\uav_img_cap\\diploma_uni_env\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Gram\\.cache\\huggingface\\hub\\models--llava-hf--llava-v1.6-mistral-7b-hf. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading shards:   0%|                                                                                                                                        | 0/4 [01:00<?, ?it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"image-text-to-text\", model=\"llava-hf/llava-v1.6-mistral-7b-hf\")\n",
    "messages = [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "          {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/ai2d-demo.jpg\"},\n",
    "          {\"type\": \"text\", \"text\": \"What does the label 15 represent? (1) lava (2) core (3) tunnel (4) ash cloud\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "out = pipe(text=messages, max_new_tokens=20)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e3700c6-7f35-4a27-9231-19b097f8216f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gram\\Desktop\\NULP\\uav_img_cap\\diploma_uni_env\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:818: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"distilgpt2\",\n",
    "    device_map=\"cpu\",\n",
    "    output_hidden_states=True# Forces the model to load on the CPU\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4176613-f765-40c1-a1c6-82a0e3471a8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install accelerate\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# Ensure the tokenizer has a padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Use eos_token as pad_token\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, tokenizer = accelerator.prepare(model, tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44b58eb3-2901-466c-b5ce-71f176a210d4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gram\\Desktop\\NULP\\uav_img_cap\\diploma_uni_env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Gram\\Desktop\\NULP\\uav_img_cap\\diploma_uni_env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\" to C:\\Users\\Gram/.cache\\torch\\hub\\checkpoints\\mobilenet_v3_small-047dcff4.pth\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9.83M/9.83M [00:01<00:00, 10.3MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MobileNetV3(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (2): Hardswish()\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
       "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
       "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n",
       "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n",
       "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (12): Conv2dNormActivation(\n",
       "      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (2): Hardswish()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=576, out_features=1024, bias=True)\n",
       "    (1): Hardswish()\n",
       "    (2): Dropout(p=0.2, inplace=True)\n",
       "    (3): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "vision_model = models.mobilenet_v3_small(pretrained=True)\n",
    "vision_model.eval()  # Set to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "097bcc9c-8b71-453f-bfbb-ee65c6b6c590",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleMultimodalPipeline(nn.Module):\n",
    "    def __init__(self, vision_model, language_model):\n",
    "        super(SimpleMultimodalPipeline, self).__init__()\n",
    "        self.vision_model = vision_model\n",
    "        self.language_model = language_model\n",
    "        self.fusion_layer = nn.Linear(1000 + 768, 768)\n",
    "\n",
    "    def forward(self, image, text_input):\n",
    "        # Extract vision features\n",
    "        vision_features = self.vision_model(image)\n",
    "        \n",
    "        # Tokenize text and extract language features\n",
    "        text_tokens = tokenizer(text_input, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        outputs = self.language_model(**text_tokens)\n",
    "        hidden_states = outputs.hidden_states[-1]\n",
    "        \n",
    "        # Combine vision and text features\n",
    "        combined_features = torch.cat((vision_features, hidden_states[:, 0, :]), dim=-1)\n",
    "        fused_output = self.fusion_layer(combined_features)\n",
    "        return fused_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83c8b22d-d97e-45b2-9d9f-14d6ae8d67f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "\n",
    "img.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "629215cf-8792-4cb3-ae33-540ac3d9d2d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "image_tensor = transform(img).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4ada6ca5-b5d8-4035-b6a0-9758a3c8644f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gram\\Desktop\\NULP\\uav_img_cap\\diploma_uni_env\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Gram\\.cache\\huggingface\\hub\\models--Salesforce--blip-image-captioning-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption: a stop sign on a street corner\n"
     ]
    }
   ],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "image_url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n",
    "image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "\n",
    "inputs = processor(image, return_tensors=\"pt\")\n",
    "output_ids = model.generate(**inputs)\n",
    "caption = processor.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"Generated Caption:\", caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd08c892-7e32-456f-82c7-1ebd1fe28838",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CausalLMOutputWithCrossAttentions' object has no attribute 'last_hidden_state'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 12\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# Tokenize the input text with padding enabled\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# text_tokens = tokenizer(text_input, return_tensors=\"pt\", padding=True, truncation=True)\u001B[39;00m\n\u001B[0;32m     11\u001B[0m image_tensor \u001B[38;5;241m=\u001B[39m image_tensor\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)  \u001B[38;5;66;03m# Move image to CPU\u001B[39;00m\n\u001B[1;32m---> 12\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mpipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtext_input\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMultimodal output:\u001B[39m\u001B[38;5;124m\"\u001B[39m, output)\n",
      "File \u001B[1;32m~\\Desktop\\NULP\\uav_img_cap\\diploma_uni_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Desktop\\NULP\\uav_img_cap\\diploma_uni_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[19], line 13\u001B[0m, in \u001B[0;36mSimpleMultimodalPipeline.forward\u001B[1;34m(self, image, text_input)\u001B[0m\n\u001B[0;32m     11\u001B[0m vision_features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvision_model(image)  \u001B[38;5;66;03m# Shape: [batch_size, 1000]\u001B[39;00m\n\u001B[0;32m     12\u001B[0m text_tokens \u001B[38;5;241m=\u001B[39m tokenizer(text_input, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, truncation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m---> 13\u001B[0m text_features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlanguage_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mtext_tokens\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlast_hidden_state\u001B[49m  \u001B[38;5;66;03m# Shape: [batch_size, seq_len, 768]\u001B[39;00m\n\u001B[0;32m     14\u001B[0m combined_features \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat((vision_features, text_features[:, \u001B[38;5;241m0\u001B[39m, :]), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     15\u001B[0m fused_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfusion_layer(combined_features)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'CausalLMOutputWithCrossAttentions' object has no attribute 'last_hidden_state'"
     ]
    }
   ],
   "source": [
    "pipeline = SimpleMultimodalPipeline(vision_model, model)\n",
    "\n",
    "text_input = \"What is the significance of this Australian stop sign?\"\n",
    "\n",
    "# text_tokens = tokenizer(text_input, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "\n",
    "image_tensor = image_tensor.to(\"cpu\")\n",
    "output = pipeline(image_tensor, text_input)\n",
    "\n",
    "print(\"Multimodal output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352bb821-3189-420e-a2c7-e7ae651d59c9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Runpod generation LLaVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd35969-65bc-4f9c-900c-0f8498ceddaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install roboflow, pillow, transformers, requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae5d9c9-7e31-4432-810c-560295d80a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\"liuhaotian/llava-v1.6-mistral-7b\")\n",
    "processor = AutoProcessor.from_pretrained(\"liuhaotian/llava-v1.6-mistral-7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b95e2fb-382a-41ec-8d86-27348e7ca0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from roboflow import Roboflow\n",
    "import glob\n",
    "                \n",
    "# Path to dataset images\n",
    "rf = Roboflow(api_key=os.getenv(\"ROBFLOW_API_KEY\"))\n",
    "project = rf.workspace(\"capstoneproject\").project(\"russian-military-annotated\")\n",
    "version = project.version(4)\n",
    "dataset = version.download(\"coco\")\n",
    "print(f\"Dataset downloaded to {dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c4c5b2-1cf1-48cc-acdb-15bfc7253e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to dataset images\n",
    "image_dir1 = os.path.join(dataset.location, \"train\")\n",
    "image_dir2 = os.path.join(dataset.location, \"test\")\n",
    "image_dir3 = os.path.join(dataset.location, \"valid\")\n",
    "\n",
    "image_files = glob.glob(f\"{image_dir1}/*.jpg\") \n",
    "image_files += glob.glob(f\"{image_dir2}/*.jpg\") \n",
    "image_files += glob.glob(f\"{image_dir3}/*.jpg\") \n",
    "\n",
    "\n",
    "# Describe an image\n",
    "def describe_image(image_path):\n",
    "    prompt = \"USER: <image>\\nDescribe the image in as much detail as possible, focusing on every element,\" \\\n",
    "             \" such as place, objects, kind of vehicles, people's clothing or uniform and any important features.\" \\\n",
    "             \" ASSISTANT:\"\n",
    "    \n",
    "    image = Image.open(image_path)\n",
    "    inputs = processor(images=image, text=prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate\n",
    "    generate_ids = model.generate(**inputs, max_new_tokens=300)\n",
    "    description = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    \n",
    "    return description\n",
    "\n",
    "# Run inference\n",
    "results = []\n",
    "\n",
    "for image_file in image_files:\n",
    "    description = describe_image(image_file)\n",
    "    results.append({\"image\": image_file, \"description\": description})    \n",
    "    break\n",
    "\n",
    "# Save results to a JSON file\n",
    "import json\n",
    "with open(\"inference_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "os.path.join(os.getcwd(), \"inference_results.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1bfedff4-7ff7-43c7-9095-e803758b89d6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[50], line 9\u001B[0m\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m subprocess\u001B[38;5;241m.\u001B[39mCalledProcessError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m      7\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError sending file: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 9\u001B[0m \u001B[43msend_from_runpod\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetcwd\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muntitled.txt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[50], line 5\u001B[0m, in \u001B[0;36msend_from_runpod\u001B[1;34m(file_path)\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21msend_from_runpod\u001B[39m(file_path):\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m----> 5\u001B[0m         \u001B[43msubprocess\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrunpodctl send\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfile_path\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m subprocess\u001B[38;5;241m.\u001B[39mCalledProcessError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m      7\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError sending file: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py:503\u001B[0m, in \u001B[0;36mrun\u001B[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001B[0m\n\u001B[0;32m    500\u001B[0m     kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstdout\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m PIPE\n\u001B[0;32m    501\u001B[0m     kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstderr\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m PIPE\n\u001B[1;32m--> 503\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Popen(\u001B[38;5;241m*\u001B[39mpopenargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;28;01mas\u001B[39;00m process:\n\u001B[0;32m    504\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    505\u001B[0m         stdout, stderr \u001B[38;5;241m=\u001B[39m process\u001B[38;5;241m.\u001B[39mcommunicate(\u001B[38;5;28minput\u001B[39m, timeout\u001B[38;5;241m=\u001B[39mtimeout)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py:971\u001B[0m, in \u001B[0;36mPopen.__init__\u001B[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001B[0m\n\u001B[0;32m    967\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtext_mode:\n\u001B[0;32m    968\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstderr \u001B[38;5;241m=\u001B[39m io\u001B[38;5;241m.\u001B[39mTextIOWrapper(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstderr,\n\u001B[0;32m    969\u001B[0m                     encoding\u001B[38;5;241m=\u001B[39mencoding, errors\u001B[38;5;241m=\u001B[39merrors)\n\u001B[1;32m--> 971\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execute_child\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexecutable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreexec_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclose_fds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    972\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mpass_fds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcwd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    973\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mstartupinfo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreationflags\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshell\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    974\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mp2cread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mp2cwrite\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    975\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mc2pread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mc2pwrite\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    976\u001B[0m \u001B[43m                        \u001B[49m\u001B[43merrread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrwrite\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    977\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mrestore_signals\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    978\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mgid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mumask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    979\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mstart_new_session\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    980\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[0;32m    981\u001B[0m     \u001B[38;5;66;03m# Cleanup if the child failed starting.\u001B[39;00m\n\u001B[0;32m    982\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mfilter\u001B[39m(\u001B[38;5;28;01mNone\u001B[39;00m, (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstdin, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstdout, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstderr)):\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py:1440\u001B[0m, in \u001B[0;36mPopen._execute_child\u001B[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001B[0m\n\u001B[0;32m   1438\u001B[0m \u001B[38;5;66;03m# Start the process\u001B[39;00m\n\u001B[0;32m   1439\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1440\u001B[0m     hp, ht, pid, tid \u001B[38;5;241m=\u001B[39m \u001B[43m_winapi\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCreateProcess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexecutable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1441\u001B[0m \u001B[43m                             \u001B[49m\u001B[38;5;66;43;03m# no special security\u001B[39;49;00m\n\u001B[0;32m   1442\u001B[0m \u001B[43m                             \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1443\u001B[0m \u001B[43m                             \u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mclose_fds\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1444\u001B[0m \u001B[43m                             \u001B[49m\u001B[43mcreationflags\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1445\u001B[0m \u001B[43m                             \u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1446\u001B[0m \u001B[43m                             \u001B[49m\u001B[43mcwd\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1447\u001B[0m \u001B[43m                             \u001B[49m\u001B[43mstartupinfo\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1448\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m   1449\u001B[0m     \u001B[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001B[39;00m\n\u001B[0;32m   1450\u001B[0m     \u001B[38;5;66;03m# handles that only the child should have open.  You need\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1453\u001B[0m     \u001B[38;5;66;03m# pipe will not close when the child process exits and the\u001B[39;00m\n\u001B[0;32m   1454\u001B[0m     \u001B[38;5;66;03m# ReadFile will hang.\u001B[39;00m\n\u001B[0;32m   1455\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_close_pipe_fds(p2cread, p2cwrite,\n\u001B[0;32m   1456\u001B[0m                          c2pread, c2pwrite,\n\u001B[0;32m   1457\u001B[0m                          errread, errwrite)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [WinError 2] The system cannot find the file specified"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def send_from_runpod(file_path):\n",
    "    try:\n",
    "        subprocess.run([\"runpodctl\", \"send\", file_path])\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error sending file: {e}\")\n",
    "        \n",
    "send_from_runpod(os.path.join(os.getcwd(), \"inference_results.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e17a81a-39e7-4c2c-9278-8d63b89f3a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def download_from_runpod(code):\n",
    "    \"\"\"\n",
    "    Download a file from RunPod instance to local machine using runpodctl.\n",
    "    \n",
    "    Parameters:\n",
    "    remote_file_path (str): Path of the file on the RunPod instance.\n",
    "    local_file_path (str): Path where the file will be saved locally.\n",
    "    \"\"\"\n",
    "    # Execute the runpodctl command to download the file\n",
    "    try:\n",
    "        subprocess.run(['runpodctl', 'recieve', code])\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error downloading file: {e}\")\n",
    "\n",
    "# Example usage\n",
    "code = ''\n",
    "download_from_runpod(code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ceeb525-f78e-45ed-8edf-95fe65b6a6c8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Runpod generation Kosmos-2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b46f4c-c944-4852-b34d-c4471eca7be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install roboflow pillow transformers requests tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4f0a71-f63f-487a-9fc9-92dc9207134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from roboflow import Roboflow\n",
    "import glob\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "prompt = (\n",
    "    \"<grounding> Describe this image in detail: \"\n",
    "    \"Provide a comprehensive caption that explains every visible object,\"\n",
    "    \" its attributes, and how objects relate spatially. \"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732ae3e5-fc71-4d4b-b8b6-090a7d82e0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to dataset images\n",
    "rf = Roboflow(api_key=os.getenv(\"ROBFLOW_API_KEY\"))\n",
    "project = rf.workspace(\"capstoneproject\").project(\"russian-military-annotated\")\n",
    "version = project.version(4)\n",
    "dataset = version.download(\"coco\")\n",
    "print(f\"Dataset downloaded to {dataset}\")\n",
    "\n",
    "image_files = []\n",
    "# Path to dataset images\n",
    "for dir_name in [\"train\", \"test\", \"valid\"]:\n",
    "    image_dir = os.path.join(dataset.location, dir_name)\n",
    "    image_files += glob.glob(f\"{image_dir}/*.jpg\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fa211c-5f8e-4919-9322-0148b02799fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoProcessor, Kosmos2ForConditionalGeneration\n",
    "prompt = (\n",
    "    \"<grounding> Describe this image in detail: \"\n",
    "    \"Provide a comprehensive caption that explains every visible object,\"\n",
    "    \" its attributes, and how objects relate spatially. \"\n",
    ")\n",
    "model = Kosmos2ForConditionalGeneration.from_pretrained(\"microsoft/kosmos-2-patch14-224\")\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/kosmos-2-patch14-224\")\n",
    "model = model.to('cuda')\n",
    "def describe_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "    inputs = {key: val.to('cuda') for key, val in inputs.items()}\n",
    "    generated_ids = model.generate(\n",
    "        pixel_values=inputs[\"pixel_values\"],\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        image_embeds=None,\n",
    "        image_embeds_position_mask=inputs[\"image_embeds_position_mask\"],\n",
    "        use_cache=True,\n",
    "        max_new_tokens=128,\n",
    "    )\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    caption, entities = processor.post_process_generation(generated_text)\n",
    "\n",
    "    return caption, entities\n",
    "\n",
    "# Run inference\n",
    "results = []\n",
    "\n",
    "for image_path in tqdm(image_files, desc=\"Processing Images\"):\n",
    "    cap, entities = describe_image(image_path)\n",
    "    res = {\"image\": image_path, \"description\": cap, \"entities\": entities}\n",
    "    results.append(res)\n",
    "    \n",
    "    if len(results)%100==0:\n",
    "        # Save results to a JSON file\n",
    "        with open(\"inference_results_kosmos_2.json\", \"w\") as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "\n",
    "with open(\"inference_results_kosmos_2.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "        \n",
    "\n",
    "os.path.join(os.getcwd(), \"inference_results_kosmos_2.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19d84062-578d-4baf-8e61-c070c66b572c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Dataset downloaded to <roboflow.core.dataset.Dataset object at 0x000001BB94FDFA60>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from roboflow import Roboflow\n",
    "import glob\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "prompt = (\n",
    "    \"<grounding> Describe this image in detail: \"\n",
    "    \"Provide a comprehensive caption that explains every visible object, its attributes, and how objects relate spatially. \"\n",
    ")\n",
    "\n",
    "\n",
    "# Path to dataset images\n",
    "rf = Roboflow(api_key=os.getenv(\"ROBFLOW_API_KEY\"))\n",
    "project = rf.workspace(\"capstoneproject\").project(\"russian-military-annotated\")\n",
    "version = project.version(4)\n",
    "dataset = version.download(\"coco\")\n",
    "print(f\"Dataset downloaded to {dataset}\")\n",
    "\n",
    "image_files = []\n",
    "# Path to dataset images\n",
    "for dir_name in [\"train\", \"test\", \"valid\"]:\n",
    "    image_dir = os.path.join(dataset.location, dir_name)\n",
    "    image_files += glob.glob(f\"{image_dir}/*.jpg\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13232938-01bf-4776-a259-f2d8b8c9ba8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(416, 416)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "image = Image.open(image_files[0]).convert(\"RGB\")\n",
    "image._size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c832795-b5c3-4723-9377-56b8b0ef1792",
   "metadata": {
    "tags": []
   },
   "source": [
    "# last step Verification and Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0baec414-401e-460d-acf1-ff8b712cca4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install langchain langchain-community huggingface_hub roboflow pillow transformers requests tqdm pandas\n",
    "# pip install -q diffusers transformers accelerate peft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdba7918-a3d1-425e-b9a2-682eb256d557",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "token=...\n",
    "login(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baadbc5-aa3d-4fcb-b198-8a272f701f34",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "af123e91-137f-4d69-8906-da2274e678de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template_llama2_general = \"\"\"\n",
    "You are an advanced language model tasked with merging detailed captions generated by two multimodal systems. Your goal is to create a unified, highly detailed, and factually accurate description of the image.\n",
    "\n",
    "### Instructions:\n",
    "- Combine the details from both captions, ensuring all critical elements (objects, actions, spatial relationships, and contextual nuances) are represented.\n",
    "- Maintain a coherent and consistent tone throughout the output.\n",
    "- Avoid contradictions or redundant details; instead, refine and enhance the combined information for clarity.\n",
    "- Incorporate additional entities provided, enriching the description with fine-grained visual and contextual details.\n",
    "- The resulting caption should serve as the most comprehensive and accurate summary of the image.\n",
    "\n",
    "### Input Captions:\n",
    "1. Caption 1: {caption1}\n",
    "2. Caption 2: {caption2}\n",
    "\n",
    "### Additional Entities:\n",
    "- {entities}\n",
    "\n",
    "### Output:\n",
    "Write a single, unified caption that captures the full essence of the image, accurately representing all provided details and entities.\n",
    "\"\"\"\n",
    "\n",
    "prompt_template_llama2_object_detection = \"\"\"\n",
    "You are an advanced language model tasked with merging detailed captions generated by two multimodal systems. Your goal is to create a unified, highly detailed, and factually accurate description of the image.  \n",
    "\n",
    "Below is a detailed description of an image. Your task is to extract and list every object mentioned in the description that can be reliably detected by an object detection model. Please follow these instructions carefully:  \n",
    "\n",
    "### Instructions:  \n",
    "1. Identify each object that is clearly described and can be recognized by typical object detection systems.  \n",
    "2. Be very cautious with numerical details attached to the objects.  \n",
    "3. Do not combine or generalize objects if different numbers are specified. Each unique instance should be explicitly listed.  \n",
    "\n",
    "### Detailed Description:  \n",
    "{description}  \n",
    "\n",
    "### Output:  \n",
    "You must return the response in **EXACTLY** this JSON format. If the response is not formatted as JSON, it will be invalid.  \n",
    "\n",
    "```json\n",
    "{\n",
    "    \"results\": [\"<list of objects>\"]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "final_caption_prompt_template = \"\"\"\n",
    "You are an advanced language model tasked with refining a detailed image caption. Your goal is to ensure that the caption is both highly detailed and free of hallucinations by cross-checking it against detection probabilities from an object detection model.\n",
    "\n",
    "### Instructions:\n",
    "1. Carefully analyze the provided detailed description of the image.\n",
    "2. For each object in the description:\n",
    "   - Cross-check its presence using the detection probabilities from the object detection model.\n",
    "   - If the probability of detection for an object is high (e.g., above 0.1), retain the object in the caption.\n",
    "   - If the probability of detection for an object is low (e.g., below 0.1), treat it as a potential hallucination and decide whether to remove it from the caption.\n",
    "   - If the object is not present in the detection probabilities, leave it unchanged in the caption.\n",
    "3. Ensure that the revised caption maintains logical flow, coherence, and a consistent tone after removing hallucinated objects.\n",
    "4. Do not add any new objects or make assumptions beyond the provided information.\n",
    "\n",
    "### Inputs:\n",
    "- Detailed Description: {description}\n",
    "- Detection Probabilities: {object_probabilities}\n",
    "\n",
    "### Output:\n",
    "Write a revised, highly detailed caption where all described objects are verified as detected and potential hallucinations are removed.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5fa76945-996f-4d2c-af20-a80a6c30f8d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "root_dir = os.getcwd()\n",
    "path_to_kosmos = os.path.join(root_dir, \"inference_results_kosmos_2.json\")\n",
    "path_to_llava = os.path.join(root_dir, \"inference_results_llava_first_step.json\")\n",
    "\n",
    "kosmos_res = pd.read_json(path_to_kosmos)\n",
    "llava_res = pd.read_json(path_to_llava)\n",
    "\n",
    "first_step_df = kosmos_res.merge(llava_res, on=\"image\", suffixes=(\"_kosmos\", \"_llava\")).drop_duplicates(subset=\"image\", keep=\"last\")\n",
    "# with open(path_to_file, \"rb\") as f:\n",
    "#     kosmos_inference_res = json.loads(f.read())\n",
    "# pd.read_json(kosmos_inference_res)\n",
    "inputs = first_step_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a100b78-de53-405a-afdc-ad6e7e0fe37c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install langchain-community\n",
    "# ! curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# typing extensions problem\n",
    "# Ollama serve first in cmd -> then you can pull the llama2:13b version you need\n",
    "# you can drag and drop files in runpod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b30c1fb-fae5-45b7-9358-628b2544ad56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gram\\Desktop\\NULP\\uav_img_cap\\diploma_uni_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "# from langchain_community.chat_models import ChatOllama\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from transformers import pipeline\n",
    "import timeit\n",
    "\n",
    "\n",
    "local_llm = \"llama2:13b\"\n",
    "model_name = \"meta-llama/Llama-2-7b\"\n",
    "\n",
    "prompt_general = PromptTemplate(template=prompt_template_llama2_general, input_variables=[\"caption1\", \"caption2\", \"entities\"])\n",
    "prompt_object_detection = PromptTemplate(template=prompt_template_llama2_object_detection, input_variables=[\"description\"])\n",
    "prompt_final = PromptTemplate(template=final_caption_prompt_template, input_variables=[\"initial_description\", \"description\", \"object_probabilities\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644b8194-fb10-47fd-b67d-2d2083de9d78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# середній час одного тексту 5сек\n",
    "\n",
    "\n",
    "# del llmM\n",
    "# first_step_df.description_llava.str.split(\"ASSISTANT:\").str[1]\n",
    "\n",
    "# Convert the formatted prompt into the expected message format\n",
    "# messages = [{\"role\": \"user\", \"content\": formatted_prompt}]\n",
    "start = timeit.timeit()\n",
    "\n",
    "# LLM\n",
    "# llm = ChatOllama(model=local_llm, temperature=0.2, num_gpu=1)\n",
    "hf_pipeline = pipeline(\"text-generation\", model=model_name, device=0)\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=hf_pipeline,\n",
    "    model_kwargs={\"temperature\": 0.2, \"max_length\": 100, \"top_k\": 50, \"top_p\": 0.95}\n",
    ")\n",
    "\n",
    "# llm = HuggingFacePipeline()\n",
    "\n",
    "first_step_llama = prompt_general | llm | StrOutputParser()\n",
    "\n",
    "first_step_responses = []\n",
    "for inputs in tqdm(first_step_df.iterrows(), desc=\"First step llama2 generation\"):\n",
    "    # Call the LLM with the correctly formatted messages\n",
    "    response_first = first_step_llama.invoke(dict(\n",
    "                                                caption1=inputs[\"description_llava\"].split(\"ASSISTANT:\")[1],\n",
    "                                                caption2=inputs[\"description_kosmos\"],\n",
    "                                                entities=\", \".join([entities[0] for entities in inputs[\"entities\"]])\n",
    "                                             ))\n",
    "    \n",
    "    first_step_responses.append({\"image\": inputs[\"image\"], \"description_llama_first\": response_first})\n",
    "    \n",
    "    if len(first_step_responses) % 100 == 0:\n",
    "        with open(\"inference_llama_first.json\", \"w\") as f:\n",
    "            json.dump(first_step_responses, f, indent=4)\n",
    "    \n",
    "with open(\"inference_llama_first.json\", \"w\") as f:\n",
    "    json.dump(first_step_responses, f, indent=4)\n",
    "\n",
    "end = timeit.timeit()\n",
    "inf_time = {}\n",
    "\n",
    "with open(\"inference_time.json\", \"wb\") as f:\n",
    "    inf_time[\"inference_llama_first_time\"] = end - start\n",
    "    json.dump(inf_time, f)\n",
    "    "
   ]
  },
  {
   "attachments": {
    "e7b609d3-8e73-49b5-9e69-cf502f92ba74.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "5b521794-d9e6-46c6-9308-0c9273ebdbcc",
   "metadata": {},
   "source": [
    "![image.png](attachment:e7b609d3-8e73-49b5-9e69-cf502f92ba74.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a41b5ad-6ff9-42eb-bdf6-c8534564a8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"inference_time.json\", \"rb\") as f:\n",
    "    inf_time = json.loads(f.read())    \n",
    "    \n",
    "with open(\"inference_llama_first.json\", \"r\") as f:\n",
    "    first_step_responses = json.loads(f.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3a84bdcd-42ec-44b7-80b1-8c6f1ad3a598",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[38], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mAutoTokenizer\u001B[49m\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_name)\n\u001B[0;32m      2\u001B[0m model \u001B[38;5;241m=\u001B[39m AutoModelForCausalLM\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_name)\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Start timer\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'AutoTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n",
    "\n",
    "# Start timer\n",
    "start = timeit.default_timer()\n",
    "\n",
    "second_step_responses = []\n",
    "\n",
    "for response_first in tqdm(first_step_responses, desc=\"Second step llama2 generation\"):\n",
    "    # Preprocess input: tokenize the description\n",
    "    print()\n",
    "    formatted_description = prompt_object_detection.format(description=response_first[\"description_llama_first\"])\n",
    "    inputs_enc = tokenizer(formatted_description, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Run inference with torch.no_grad() to save memory\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            inputs_enc[\"input_ids\"],\n",
    "            max_new_tokens=100,  # Control the number of tokens generated\n",
    "            temperature=0.5,\n",
    "            top_k=50,\n",
    "            top_p=0.95\n",
    "        )\n",
    "\n",
    "    # Decode the generated response\n",
    "    response_second = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Append the results with image info\n",
    "    second_step_responses.append({\n",
    "        \"image\": response_first[\"image\"],\n",
    "        \"objects_llama_second\": response_second  # Use the decoded text as the result\n",
    "    })\n",
    "\n",
    "    # Save results in batches\n",
    "    if len(second_step_responses) % 100 == 0:\n",
    "        with open(\"inference_llama_second.json\", \"w\") as f:\n",
    "            json.dump(second_step_responses, f, indent=4)\n",
    "\n",
    "# Final result saving\n",
    "with open(\"inference_llama_second.json\", \"w\") as f:\n",
    "    json.dump(second_step_responses, f, indent=4)\n",
    "\n",
    "# End timer\n",
    "end = timeit.default_timer()\n",
    "\n",
    "# Save inference time\n",
    "inf_time = {}\n",
    "with open(\"inference_time.json\", \"w\") as f:\n",
    "    inf_time[\"inference_llama_second_time\"] = end - start\n",
    "    json.dump(inf_time, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "554f666c-d299-4569-8a8e-ba2f88027840",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an advanced language model tasked with merging detailed captions generated by two multimodal systems. Your goal is to create a unified, highly detailed, and factually accurate description of the image.\n",
      "\n",
      "Below is a detailed description of an image. Your task is to extract and list every object mentioned in the description that can be reliably detected by an object detection model. Please follow these instructions carefully:\n",
      "\n",
      "### Instructions:\n",
      "1. Identify each object that is clearly described and can be recognized by typical object detection systems.\n",
      "2. Be very cautious with numerical details attached to the objects.\n",
      "3. Do not combine or generalize objects if different numbers are specified. Each unique instance should be explicitly listed.\n",
      "4. **Return only the JSON object in the exact format provided. Your response must contain only the JSON code block as specified. Do not wrap it in markdown, provide any explanations, or include any other content.**\n",
      "\n",
      "### Detailed Description:\n",
      " The image shows a group of military personnel walking down a dirt road in a rural area. There are at least six people visible in the scene, with some of them carrying backpacks. The soldiers are wearing uniforms, and some of them are equipped with weapons, such as a rifle and a handgun.\n",
      "\n",
      "In addition to the military personnel, there are two trucks in the scene, one located near the center of the image and the other towards the right side. These trucks are likely used for transportation and logistics purposes during military operations. The overall atmosphere of the image suggests a military patrol or training exercise in a rural setting. Describe this image in detail: Provide a comprehensive caption that explains every visible object, its attributes, and how objects relate spatially. In the image, there is a dirt road running through a corn field, with the sun setting in the background. The road is surrounded by tall corn plants, creating a picturesque scene. The corn plants are tall and spread across the field, creating an interesting contrast with the dirt road. The road is lined with trees, providing shade and a sense of privacy. The trees are also a reminder of the natural beauty of the area, despite the presence of the military personnel. The soldiers are walking down the road, with some of them carrying backpacks and weapons. The soldiers are wearing uniforms and appear to be on a mission or training exercise. The trucks are parked on the side of the road, with one of them being a military vehicle\n",
      "\n",
      "### Output:\n",
      "You must return the response in **EXACTLY** this JSON format. If the response is not formatted as JSON, it will be invalid.\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"results\": [\"<list of objects>\"]\n",
      "}\n",
      "```\n",
      "\n",
      "**Note:** The list of objects should be a JSON array containing the object descriptions in the following format:\n",
      "```json\n",
      "{\n",
      "    \"object\": \"<object name>\",\n",
      "    \"type\": \"<object type>\",\n",
      "    \"attributes\": {\n",
      "        \"<attribute name>\": \"<attribute value>\"\n",
      "    },\n",
      "    \"spatial\": {\n",
      "        \"<spatial coordinate>\": \"<spatial coordinate value>\"\n",
      "    }\n",
      "}\n",
      "```\n",
      "For example, the description of the first soldier in the list might look like this:\n",
      "```json\n",
      "{\n",
      "    \"object\": \"Soldier 1\",\n",
      "    \"type\": \"Person\",\n",
      "    \"attributes\": {\n",
      "        \"age\": \"Adult\",\n",
      "        \"gender\": \"Male\"\n",
      "    },\n",
      "    \"spatial\": {\n",
      "        \"top\": {\n",
      "            \"x\": 0.5,\n",
      "            \"y\": 0.5\n",
      "        },\n",
      "        \"left\": {\n",
      "            \"x\": 0.25,\n",
      "            \"y\": 0.25\n",
      "        },\n",
      "        \"right\": {\n",
      "            \"x\": 0.75,\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[44], line 17\u001B[0m\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;66;03m# json_str = re.search(r'\\{.*\\}', , re.DOTALL).group(0)\u001B[39;00m\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;66;03m# match = re.search(r'(\\{.*\\})', output, re.DOTALL)\u001B[39;00m\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;66;03m# matches = re.findall(pattern, output, re.DOTALL)\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \n\u001B[0;32m     15\u001B[0m     \u001B[38;5;66;03m# try:\u001B[39;00m\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;28mprint\u001B[39m(output)\n\u001B[1;32m---> 17\u001B[0m     res \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mloads(\u001B[43mmatches\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m)\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m     19\u001B[0m     \u001B[38;5;66;03m# print(res)\u001B[39;00m\n\u001B[0;32m     20\u001B[0m     \u001B[38;5;66;03m# except Exception as e:\u001B[39;00m\n\u001B[0;32m     21\u001B[0m     \u001B[38;5;66;03m#     print(e)\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;66;03m# data = json.loads(json_str)\u001B[39;00m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;66;03m# print(data)\u001B[39;00m\n",
      "\u001B[1;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Assume 'output' is the string returned by the model.\n",
    "# This regex extracts the first JSON object found.\n",
    "pattern = r'json\\s*(\\{.*?\\})\\s*'\n",
    "\n",
    "for res in json.loads(open(\"../processed_data/inference_llama_second.json\").read()):\n",
    "    output = res[\"objects_llama_second\"]\n",
    "    # json_str = re.search(r'\\{.*\\}', , re.DOTALL).group(0)\n",
    "    # match = re.search(r'(\\{.*\\})', output, re.DOTALL)\n",
    "    # matches = re.findall(pattern, output, re.DOTALL)\n",
    "\n",
    "    # try:\n",
    "    print(output)\n",
    "    res = json.loads(matches[1])\n",
    "    break\n",
    "    # print(res)\n",
    "    # except Exception as e:\n",
    "    #     print(e)\n",
    "    #     print(output)\n",
    "    #     break\n",
    "# if match:\n",
    "#     json_str = match.group(1)\n",
    "#     print(json_str)\n",
    "#     data = json.loads(json_str)\n",
    "#     print(data)\n",
    "# else:\n",
    "#     print(\"No JSON object found.\")\n",
    "    \n",
    "# data = json.loads(json_str)\n",
    "# print(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ac61ca8-566b-47d8-95eb-9028be888c2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [INST] <<SYS>>\n",
      "You are an advanced language model tasked with merging detailed captions generated by two multimodal systems. Your goal is to create a unified, highly detailed, and factually accurate description of the image.\n",
      "\n",
      "Below is a detailed description of an image. Your task is to extract and list every object mentioned in the description that can be reliably detected by an object detection model. Please follow these instructions carefully:\n",
      "\n",
      "### Instructions:\n",
      "1. Identify each object that is clearly described and can be recognized by typical object detection systems.\n",
      "2. Be very cautious with numerical details attached to the objects.\n",
      "3. Do not combine or generalize objects if different numbers are specified. Each unique instance should be explicitly listed.\n",
      "4. **Return only the JSON object in the exact format provided. Your response must contain only the JSON code block as specified. Do not wrap it in markdown, provide any explanations, or include any other content.**\n",
      "\n",
      "### Output:\n",
      "You must return the response in **EXACTLY** this JSON format. If the response is not formatted as JSON, it will be invalid.\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"results\": [\"<list of objects>\"]\n",
      "}\n",
      "<</SYS>>\n",
      "Description:  The image shows a group of military personnel walking down a dirt road in a rural area. There are at least six people visible in the scene, with some of them carrying backpacks. The soldiers are wearing uniforms, and some of them are equipped with weapons, such as a rifle and a handgun.\n",
      "\n",
      "In addition to the military personnel, there are two trucks in the scene, one located near the center of the image and the other towards the right side. These trucks are likely used for transportation and logistics purposes during military operations. The overall atmosphere of the image suggests a military patrol or training exercise in a rural setting. Describe this image in detail: Provide a comprehensive caption that explains every visible object, its attributes, and how objects relate spatially. In the image, there is a dirt road running through a corn field, with the sun setting in the background. The road is surrounded by tall corn plants, creating a picturesque scene. The corn plants are tall and spread across the field, creating an interesting contrast with the dirt road. The road is lined with trees, providing shade and a sense of privacy. The trees are also a reminder of the natural beauty of the area, despite the presence of the military personnel. The soldiers are walking down the road, with some of them carrying backpacks and weapons. The soldiers are wearing uniforms and appear to be on a mission or training exercise. The trucks are parked on the side of the road, with one of them being a military vehicle\n",
      "[/INST]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a2d245e2-aa61-48fc-860c-8b2e9414ea4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an advanced language model tasked with merging detailed captions generated by two multimodal systems. Your goal is to create a unified, highly detailed, and factually accurate description of the image.\n",
      "\n",
      "Below is a detailed description of an image. Your task is to extract and list every object mentioned in the description that can be reliably detected by an object detection model. Please follow these instructions carefully:\n",
      "\n",
      "### Instructions:\n",
      "1. Identify each object that is clearly described and can be recognized by typical object detection systems.\n",
      "2. Be very cautious with numerical details attached to the objects.\n",
      "3. Do not combine or generalize objects if different numbers are specified. Each unique instance should be explicitly listed.\n",
      "4. **Return only the JSON object in the exact format provided. Your response must contain only the JSON code block as specified. Do not wrap it in markdown, provide any explanations, or include any other content.**\n",
      "\n",
      "### Detailed Description:\n",
      " The image shows a group of military personnel walking down a dirt road in a rural area. There are at least six people visible in the scene, with some of them carrying backpacks. The soldiers are wearing uniforms, and some of them are equipped with weapons, such as a rifle and a handgun.\n",
      "\n",
      "In addition to the military personnel, there are two trucks in the scene, one located near the center of the image and the other towards the right side. These trucks are likely used for transportation and logistics purposes during military operations. The overall atmosphere of the image suggests a military patrol or training exercise in a rural setting. Describe this image in detail: Provide a comprehensive caption that explains every visible object, its attributes, and how objects relate spatially. In the image, there is a dirt road running through a corn field, with the sun setting in the background. The road is surrounded by tall corn plants, creating a picturesque scene. The corn plants are tall and spread across the field, creating an interesting contrast with the dirt road. The road is lined with trees, providing shade and a sense of privacy. The trees are also a reminder of the natural beauty of the area, despite the presence of the military personnel. The soldiers are walking down the road, with some of them carrying backpacks and weapons. The soldiers are wearing uniforms and appear to be on a mission or training exercise. The trucks are parked on the side of the road, with one of them being a military vehicle\n",
      "\n",
      "### Output:\n",
      "You must return the response in **EXACTLY** this JSON format. If the response is not formatted as JSON, it will be invalid.\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"results\": [\"<list of objects>\"]\n",
      "}\n",
      "```\n",
      "\n",
      "**Note:** The list of objects should be a JSON array containing the object descriptions in the following format:\n",
      "```json\n",
      "{\n",
      "    \"object\": \"<object name>\",\n",
      "    \"type\": \"<object type>\",\n",
      "    \"attributes\": {\n",
      "        \"<attribute name>\": \"<attribute value>\"\n",
      "    },\n",
      "    \"spatial\": {\n",
      "        \"<spatial coordinate>\": \"<spatial coordinate value>\"\n",
      "    }\n",
      "}\n",
      "```\n",
      "For example, the description of the first soldier in the list might look like this:\n",
      "```json\n",
      "{\n",
      "    \"object\": \"Soldier 1\",\n",
      "    \"type\": \"Person\",\n",
      "    \"attributes\": {\n",
      "        \"age\": \"Adult\",\n",
      "        \"gender\": \"Male\"\n",
      "    },\n",
      "    \"spatial\": {\n",
      "        \"top\": {\n",
      "            \"x\": 0.5,\n",
      "            \"y\": 0.5\n",
      "        },\n",
      "        \"left\": {\n",
      "            \"x\": 0.25,\n",
      "            \"y\": 0.25\n",
      "        },\n",
      "        \"right\": {\n",
      "            \"x\": 0.75,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(json.loads(open(\"../processed_data/inference_llama_second.json\").read())[0][\"objects_llama_second\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc1caed-649b-403e-91af-acff05a96e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import timeit\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# # Initialize the tokenizer and model, explicitly transferring them to GPU\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n",
    "\n",
    "\n",
    "# LLM\n",
    "# hf_pipeline = pipeline(\"text-generation\", model=model_name, device=0)\n",
    "# llm = HuggingFacePipeline(\n",
    "#     pipeline=hf_pipeline,\n",
    "#     model_kwargs={\"temperature\": 0.5, \"max_new_tokens\": 100, \"top_k\": 50, \"top_p\": 0.95}\n",
    "# )\n",
    "\n",
    "\n",
    "# llm = ChatOllama(model=local_llm, temperature=0.5, format=\"json\", num_gpu=1)\n",
    "# second_step_llama = prompt_object_detection | llm | JsonOutputParser()\n",
    "\n",
    "second_step_responses = []\n",
    "\n",
    "# Start timer\n",
    "start = timeit.default_timer()\n",
    "\n",
    "second_step_responses = []\n",
    "end_i = 0\n",
    "for i, response_first in tqdm(first_step_responses, desc=\"Second step llama2 generation\"):\n",
    "    # Preprocess input: tokenize the description\n",
    "    formatted_description = prompt_object_detection.format(description=response_first[\"description_llama_first\"])\n",
    "    inputs_enc = tokenizer(formatted_description, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Run inference with torch.no_grad() to save memory\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            inputs_enc[\"input_ids\"],\n",
    "            max_new_tokens=100,  # Control the number of tokens generated\n",
    "            temperature=0.5,\n",
    "            top_k=50,\n",
    "            top_p=0.95\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        parsed_output = json.loads(response_second)  # Parse the JSON string into a Python dictionary\n",
    "    except json.JSONDecodeError as e:    \n",
    "        with open(\"inference_llama_second.json\", \"w\") as f:\n",
    "            json.dump(second_step_responses, f, indent=4)\n",
    "        raise e\n",
    "        \n",
    "    \n",
    "        \n",
    "    # Decode the generated response\n",
    "    response_second = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Append the results with image info\n",
    "    second_step_responses.append({\n",
    "        \"image\": response_first[\"image\"],\n",
    "        \"objects_llama_second\": response_second  # Use the decoded text as the result\n",
    "    })\n",
    "\n",
    "    # Save results in batches\n",
    "    if len(second_step_responses) % 100 == 0:\n",
    "        with open(\"inference_llama_second.json\", \"w\") as f:\n",
    "            json.dump(second_step_responses, f, indent=4)\n",
    "\n",
    "# Final result saving\n",
    "with open(\"inference_llama_second.json\", \"w\") as f:\n",
    "    json.dump(second_step_responses, f, indent=4)\n",
    "\n",
    "# End timer\n",
    "end = timeit.default_timer()\n",
    "\n",
    "# Save inference time\n",
    "inf_time = {}\n",
    "with open(\"inference_time.json\", \"w\") as f:\n",
    "    inf_time[\"inference_llama_second_time\"] = end - start\n",
    "    json.dump(inf_time, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc2c5bac-996f-4b0a-94b7-da3917ff5547",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'parse_json' from 'langchain.output_parsers' (C:\\Users\\Gram\\Desktop\\NULP\\uav_img_cap\\diploma_uni_env\\lib\\site-packages\\langchain\\output_parsers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moutput_parsers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m parse_json\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'parse_json' from 'langchain.output_parsers' (C:\\Users\\Gram\\Desktop\\NULP\\uav_img_cap\\diploma_uni_env\\lib\\site-packages\\langchain\\output_parsers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import parse_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac935309-06b1-411f-8091-1c1d60a03e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del hf_pipeline\n",
    "del llm\n",
    "del second_step_llama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0384cdb8-ecf5-442e-b4c4-712ea2b68dbe",
   "metadata": {
    "tags": []
   },
   "source": [
    "## CLIPViT object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "94f594a6-c210-499e-b382-1fa8bd2994e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import requests\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "start = timeit.timeit()\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "verification_responses = []\n",
    "\n",
    "for i, inputs in tqdm(second_step_responses, desc=\"Objects verification with CLIPViT\"):\n",
    "    texts = inputs[\"results\"]\n",
    "    image = Image.open(inputs[\"image\"])\n",
    "    inputs_processed = processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\n",
    "    inputs = {key: val.to('cuda') for key, val in inputs_processed.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs_processed)\n",
    "\n",
    "    logits_per_image = outputs.logits_per_image\n",
    "    probs = logits_per_image.softmax(dim=1)\n",
    "\n",
    "    res = list(zip(texts, probs.numpy().round(3).tolist()[0]))\n",
    "    verification_responses.append({\"image\": inputs[\"image\"], \"verification_clip_probs\": res})\n",
    "    \n",
    "    if len(verification_responses) % 100 == 0:\n",
    "        with open(\"inference_verification_clip.json\", \"w\") as f:\n",
    "            json.dump(verification_responses, f, indent=4)\n",
    "            \n",
    "end = timeit.timeit()\n",
    "\n",
    "with open(\"inference_verification_clip.json\", \"w\") as f:\n",
    "    json.dump(verification_responses, f, indent=4)\n",
    "    \n",
    "with open(\"inference_time.json\", \"wb\") as f:\n",
    "    inf_time[\"inference_verification_clip_time\"] = end - start\n",
    "    json.dump(inf_time, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "085282bb-0155-4bab-a6bf-a71542803526",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import requests\n",
    "# from transformers import CLIPProcessor, CLIPModel\n",
    "# model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "# processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "# url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n",
    "# inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(**inputs)\n",
    "# logits_per_image = outputs.logits_per_image # this is the image-text similarity score\n",
    "# probs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n",
    "\n",
    "# probs.numpy().round(3)\n",
    "\n",
    "# image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691f23d5-7f27-46a3-ae78-f228cc3e65a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Last caption Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "57297b8d-ef73-4066-8399-70baeaced70d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = timeit.timeit()\n",
    "\n",
    "final_df = pd.DataFrame(first_step_responses).merge(second_step_responses, on=\"image\").drop_duplicates(subset=[\"image\"], keep=\"last\")\n",
    "\n",
    "# LLM\n",
    "# llm = ChatOllama(model=local_llm, temperature=0, num_gpu=1)\n",
    "hf_pipeline = pipeline(\"text-generation\", model=model_name, device=0)\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=hf_pipeline,\n",
    "    model_kwargs={\"temperature\": 0, \"max_new_tokens\": 100, \"top_k\": 50, \"top_p\": 0.95}\n",
    ")\n",
    "\n",
    "final_llama = prompt_final | llm | StrOutputParser()\n",
    "\n",
    "final_responses = []\n",
    "\n",
    "for final_row in tqdm(final_df.iterrows(),  desc=\"final step llama2 captions\"):\n",
    "    response_final = final_llama.invoke({ \"description\": final_row[\"description_llama_first\"],\n",
    "                                          \"object_probabilities\": final_row[\"objects_llama_second\"]})\n",
    "    \n",
    "    final_responses.append({\"image\": response_final[\"image\"], \"final_caption_llama\": response_final})\n",
    "    if len(final_responses) % 100 == 0:\n",
    "        with open(\"inference_llama_final.json\", \"w\") as f:\n",
    "            json.dump(final_responses, f, indent=4)\n",
    "    \n",
    "with open(\"inference_llama_final.json\", \"w\") as f:\n",
    "    json.dump(final_responses, f, indent=4)\n",
    "    \n",
    "    \n",
    "end = timeit.timeit()\n",
    "\n",
    "with open(\"inference_time.json\", \"wb\") as f:\n",
    "    inf_time[\"inference_llama_final_time\"] = end - start\n",
    "    json.dump(inf_time, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab30550-851f-4510-ae48-1b4de0a037db",
   "metadata": {},
   "source": [
    "# Trying to run second llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b23ca27-a6e3-441c-b862-a594908144de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "import re\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import timeit\n",
    "# import json\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # # Initialize the tokenizer and model, explicitly transferring them to GPU\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n",
    "\n",
    "# second_step_responses = []\n",
    "\n",
    "# Start timer\n",
    "start = timeit.default_timer()\n",
    "\n",
    "second_step_responses = []\n",
    "end_i = 0\n",
    "for i, response_first in tqdm(enumerate(first_step_responses), desc=\"Second step llama2 generation\"):\n",
    "    # Preprocess input: tokenize the description\n",
    "    formatted_description = prompt_template_llama2_object_detection.format(description=response_first['description_llama_first'])\n",
    "    # formatted_description = prompt_object_detection.replace({}).format(description=response_first[\"description_llama_first\"])\n",
    "    inputs_enc = tokenizer(formatted_description, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Run inference with torch.no_grad() to save memory\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            inputs_enc[\"input_ids\"],\n",
    "            max_new_tokens=250,  # Control the number of tokens generated\n",
    "            temperature=0.5,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "        )\n",
    "        \n",
    "    # Decode the generated response\n",
    "    response_second = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Append the results with image info\n",
    "    second_step_responses.append({\n",
    "        \"image\": response_first[\"image\"],\n",
    "        \"objects_llama_second\": response_second  # Use the decoded text as the result\n",
    "    })\n",
    "\n",
    "    # Save results in batches\n",
    "    if len(second_step_responses) % 100 == 0:\n",
    "        with open(\"inference_llama_second.json\", \"w\") as f:\n",
    "            json.dump(second_step_responses, f, indent=4)\n",
    "\n",
    "# Final result saving\n",
    "with open(\"inference_llama_second.json\", \"w\") as f:\n",
    "    json.dump(second_step_responses, f, indent=4)\n",
    "\n",
    "# End timer\n",
    "end = timeit.default_timer()\n",
    "\n",
    "# Save inference time\n",
    "inf_time = {}\n",
    "with open(\"inference_time.json\", \"w\") as f:\n",
    "    inf_time[\"inference_llama_second_time\"] = end - start\n",
    "    json.dump(inf_time, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a5e26d-0dec-46f4-b8a6-fdee0c08333d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# figure out prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdfb799e-fdaa-4996-8141-460036a066be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "prompt_template_llama2_object_detection = \"\"\"\n",
    "You are an advanced language model tasked with merging detailed captions generated by two multimodal systems. Your goal is to create a unified, highly detailed, and factually accurate description of the image.\n",
    "\n",
    "I will provide you also detailed description of an image. Your task is to extract and list every object mentioned in the description that can be reliably detected by an object detection model. Please follow these instructions carefully:\n",
    "\n",
    "1. Identify each object that is clearly described and can be recognized by typical object detection systems.\n",
    "2. Be very cautious with numerical details attached to the objects.\n",
    "3. Do not combine or generalize objects if different numbers are specified. Each unique instance should be explicitly listed.\n",
    "4. **Return only the JSON object in the exact format provided. Your response must contain only the JSON code block as specified. Do not wrap it in markdown, provide any explanations, or include any other content.**\n",
    "\n",
    "You must return the response in **EXACTLY** this JSON format. If the response is not formatted as JSON, it will be invalid.\n",
    "```json\n",
    "{{\n",
    "    \"results\": [\"<list of objects>\"]\n",
    "}}\n",
    "\"\"\"\n",
    "descr_prompt = lambda x: f\"Description: {x}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "280feb25-57c9-4e6b-99fb-f50bf954d047",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "B_INST, E_INST = \"<s>[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\", \"<</SYS>>\"\n",
    "# descr = ...\n",
    "B_INST + B_SYS + prompt_template_llama2_object_detection + E_SYS + descr_prompt(descr) + E_INST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81edf1a7-9bd4-46a3-882a-1550d9404a05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an advanced language model tasked with merging detailed captions generated by two multimodal systems. Your goal is to create a unified, highly detailed, and factually accurate description of the image.\n",
      "\n",
      "I will provide you also detailed description of an image. Your task is to extract and list every object mentioned in the description that can be reliably detected by an object detection model. Please follow these instructions carefully:\n",
      "\n",
      "1. Identify each object that is clearly described and can be recognized by typical object detection systems.\n",
      "2. Be very cautious with numerical details attached to the objects.\n",
      "3. Do not combine or generalize objects if different numbers are specified. Each unique instance should be explicitly listed.\n",
      "4. **Return only the JSON object in the exact format provided. Your response must contain only the JSON code block as specified. Do not wrap it in markdown, provide any explanations, or include any other content.**\n",
      "\n",
      "You must return the response in **EXACTLY** this JSON format. If the response is not formatted as JSON, it will be invalid.\n",
      "```json\n",
      "{{\n",
      "    \"results\": [\"<list of objects>\"]\n",
      "}}\n",
      " Description:  The image shows a group of military personnel walking down a dirt road in a rural area. There are at least six people visible in the scene, with some of them carrying backpacks. The soldiers are wearing uniforms, and some of them are equipped with weapons, such as a rifle and a handgun.\n",
      "\n",
      "In addition to the military personnel, there are two trucks in the scene, one located near the center of the image and the other towards the right side. These trucks are likely used for transportation and logistics purposes during military operations. The overall atmosphere of the image suggests a military patrol or training exercise in a rural setting. Describe this image in detail: Provide a comprehensive caption that explains every visible object, its attributes, and how objects relate spatially. In the image, there is a dirt road running through a corn field, with the sun setting in the background. The road is surrounded by tall corn plants, creating a picturesque scene. The corn plants are tall and spread across the field, creating an interesting contrast with the dirt road. The road is lined with trees, providing shade and a sense of privacy. The trees are also a reminder of the natural beauty of the area, despite the presence of the military personnel. The soldiers are walking down the road, with some of them carrying backpacks and weapons. The soldiers are wearing uniforms and appear to be on a mission or training exercise. The trucks are parked on the side of the road, with one of them being a military vehicle. The trucks are also likely used for transportation and logistics purposes, such as storing supplies and military equipment.\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "### Questions\n",
      "\n",
      "- 🏁 How do you retrieve the `data` from the `results`?\n",
      "\n",
      "  - This is a common problem in many applications. Instead of relying on the object detection models themselves, you should use a more powerful and generic API that will take care of the details of the image.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import timeit\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "    model_name = \"TinyLlama/TinyLlama-1.1B-step-50K-105b\"\n",
    "\n",
    "# # Initialize the tokenizer and model, explicitly transferring them to GPU\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "B, E = \"<s>\", \"</s>\"\n",
    "descr = first_step_responses[0][\"description_llama_first\"]\n",
    "# tokenizer.tokenize(, return=\"pt\")\n",
    "input_ids = tokenizer(B + prompt_template_llama2_object_detection + E + descr_prompt(descr), return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Generate text with the model\n",
    "output_ids = model.generate(input_ids, max_new_tokens=100, num_return_sequences=1, do_sample=True, temperature=0.7)\n",
    "\n",
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39276ee1-7bcc-4443-ab09-3073a3a1c207",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"../processed_data/inference_llama_first.json\", \"r\") as f:\n",
    "    first_step_responses = json.loads(f.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b20ff92d-11b3-4a65-8b9f-f688757e267e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['inference_llama_first.json',\n",
       " 'inference_llama_second.json',\n",
       " 'inference_results_kosmos_2.json',\n",
       " 'inference_results_llava_first_step.json']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.listdir(\"../processed_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3b1d8f2-f522-4406-8fff-fa23f1152e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../processed_data/inference_results_llava_first_step.json\", \"r\") as f:\n",
    "    first_step_responses = json.loads(f.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66197b0a-e06a-46f5-98c5-2d0b6d7673be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08105f8c-eb09-4265-84fb-2d54b4ca643b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uav_img_cap",
   "language": "python",
   "name": "uav_img_cap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}