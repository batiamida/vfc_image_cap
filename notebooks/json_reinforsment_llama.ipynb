{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07c7672f-2e7c-4a46-b24c-2d28ac7dc3ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c23a4848-85a6-4f11-946d-2220846d0d7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "940200a9-3e3b-48f4-87be-8bcccb38fd4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install langchain-groq\n",
    "import os\n",
    "\n",
    "# os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fc26ae9-7ad2-43e2-9f3c-5bb0b187198f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from langchain.tools import tool\n",
    "# from langchain_core.messages import HumanMessage\n",
    "# from langgraph.checkpoint.memory import MemorySaver\n",
    "# from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "from langchain.prompts import PromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "from langchain_core.output_parsers import JsonOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a672288e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gram\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:3579: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "import time\n",
    "\n",
    "\n",
    "class ObjectsDetected(BaseModel):\n",
    "    objects_detected: List[str] = Field(description=\"List of detected objects\")\n",
    "\n",
    "        \n",
    "llm = ChatGroq(model=\"llama3-70b-8192\", temperature=0.5)\n",
    "template = '''\n",
    "You are an advanced language model tasked with merging detailed captions generated by two multimodal systems. Your goal is to create a unified, highly detailed, and factually accurate description of the image.  \n",
    "\n",
    "Below is a detailed description of an image. Your task is to extract and list every object mentioned in the description that can be reliably detected by an object detection model. Please follow these instructions carefully:  \n",
    "\n",
    "### Instructions:  \n",
    "1. Identify each object that is clearly described and can be recognized by typical object detection systems.  \n",
    "2. Be very cautious with numerical details attached to the objects, don't avoid them.  \n",
    "3. Do not combine or generalize objects if different numbers are specified. Each unique instance should be explicitly listed.  \n",
    "'''\n",
    "system_message = SystemMessagePromptTemplate.from_template(template)\n",
    "human_message = HumanMessagePromptTemplate.from_template(\"Here is description: {description}\")\n",
    "\n",
    "prompt_object_detection = ChatPromptTemplate.from_messages([system_message, human_message])\n",
    "model = prompt_object_detection | llm.with_structured_output(ObjectsDetected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f727cac6-98d6-4805-ae68-6eb3109cbe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_counter = 0\n",
    "last_reset_time = time.time()\n",
    "\n",
    "\n",
    "def invoke_with_rate_limit_per_minute(description, requests_per_minute = 29):\n",
    "    global request_counter, last_reset_time\n",
    "\n",
    "    current_time = time.time()\n",
    "    if current_time - last_reset_time >= 60:\n",
    "        request_counter = 0\n",
    "        last_reset_time = current_time\n",
    "\n",
    "    if request_counter >= requests_per_minute:\n",
    "        wait_time = time_window - (current_time - last_reset_time)\n",
    "        print(f\"Rate limit reached. Please wait {wait_time:.2f} seconds.\")\n",
    "        time.sleep(wait_time)  # Sleep until the next allowed time\n",
    "        request_counter = 0  # Reset the counter after waiting\n",
    "        last_reset_time = time.time()\n",
    "\n",
    "    # Now make the request\n",
    "    response = model.invoke({\"description\": description}).objects_detected\n",
    "\n",
    "    # Increment the request counter after the request\n",
    "    request_counter += 1\n",
    "\n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "first_llama_inference = {}\n",
    "\n",
    "result = invoke_with_rate_limit_per_minute(\"there are 3 cars are driving on dirty route\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4f4a2c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cars', 'route']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8bb6a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: regenerate first_llama_responses with groq 80b llama\n",
    "class DescriptionObject(BaseModel):\n",
    "    description: str = Field(description=\"summarized description\")\n",
    "\n",
    "        \n",
    "prompt_template_llama2_general = \"\"\"\n",
    "You are an advanced language model tasked with merging detailed captions generated by two multimodal systems. Your goal is to create a unified, highly detailed, and factually accurate description of the image.\n",
    "\n",
    "### Instructions:\n",
    "- Combine the details from both captions, ensuring all critical elements (objects, actions, spatial relationships, and contextual nuances) are represented.\n",
    "- Maintain a coherent and consistent tone throughout the output.\n",
    "- Avoid contradictions or redundant details; instead, refine and enhance the combined information for clarity.\n",
    "- Incorporate additional entities provided, enriching the description with fine-grained visual and contextual details.\n",
    "- The resulting caption should serve as the most comprehensive and accurate summary of the image.\n",
    "\"\"\"\n",
    "\n",
    "template = '''\n",
    "You are an advanced language model tasked with merging detailed captions generated by two multimodal systems. Your goal is to create a unified, highly detailed, and factually accurate description of the image.  \n",
    "\n",
    "Below is a detailed description of an image. Your task is to extract and list every object mentioned in the description that can be reliably detected by an object detection model. Please follow these instructions carefully:  \n",
    "\n",
    "### Instructions:  \n",
    "1. Identify each object that is clearly described and can be recognized by typical object detection systems.  \n",
    "2. Be very cautious with numerical details attached to the objects, don't avoid them.  \n",
    "3. Do not combine or generalize objects if different numbers are specified. Each unique instance should be explicitly listed.  \n",
    "'''\n",
    "system_message = SystemMessagePromptTemplate.from_template(prompt_template_llama2_general)\n",
    "human_message = HumanMessagePromptTemplate.from_template(\"\"\"\n",
    "### Input Captions:\n",
    "1. Caption 1: {caption1}\n",
    "2. Caption 2: {caption2}\n",
    "\n",
    "### Additional Entities:\n",
    "- {entities}\"\"\")\n",
    "\n",
    "prompt_object_detection = ChatPromptTemplate.from_messages([system_message, human_message])\n",
    "model = prompt_object_detection | llm.with_structured_output(DescriptionObject)\n",
    "\n",
    "request_counter = 0\n",
    "last_reset_time = time.time()\n",
    "\n",
    "\n",
    "def invoke_with_rate_limit_per_minute(caption1, caption2, entities, requests_per_minute = 29):\n",
    "    global request_counter, last_reset_time\n",
    "\n",
    "    current_time = time.time()\n",
    "    if current_time - last_reset_time >= 60:\n",
    "        request_counter = 0\n",
    "        last_reset_time = current_time\n",
    "\n",
    "    if request_counter >= requests_per_minute:\n",
    "        wait_time = time_window - (current_time - last_reset_time)\n",
    "        print(f\"Rate limit reached. Please wait {wait_time:.2f} seconds.\")\n",
    "        time.sleep(wait_time)\n",
    "        request_counter = 0\n",
    "        last_reset_time = time.time()\n",
    "\n",
    "    # Now make the request\n",
    "    response = model.invoke({\"caption1\": caption1, \"caption2\": caption2, \"entities\": entities}).description\n",
    "\n",
    "    # Increment the request counter after the request\n",
    "    request_counter += 1\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "386a3d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../processed_data/inference_results_llava_first_step.json\", \"r\") as f:\n",
    "    inference_llava = json.loads(f.read())\n",
    "with open(\"../processed_data/inference_results_kosmos_2.json\", \"r\") as f:\n",
    "    inference_kosmos = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23e17453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "merged_df = pd.DataFrame(inference_llava).merge(pd.DataFrame(inference_kosmos), on=\"image\", suffixes=(\"_llava\", \"_kosmos\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "88a4af91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "609"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with open(\"inference_llama_first_80b.json\", \"r\") as f:\n",
    "        first_llama_inference = json.loads(f.read())\n",
    "\n",
    "checked_images = pd.DataFrame(first_llama_inference).image.unique().tolist()\n",
    "len(checked_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a5995564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i, r in tqdm(merged_df[~merged_df.image.isin(checked_images)].iterrows(), total=(~merged_df.image.isin(checked_images)).sum()):\n",
    "    entities = list(map(lambda entity: entity[0], r[\"entities\"]))\n",
    "    try:\n",
    "        first_llama_inference.append({\"image\": r[\"image\"], \"description\": invoke_with_rate_limit_per_minute(r[\"description_llava\"], r[\"description_kosmos\"], entities)})\n",
    "    except:\n",
    "        time.sleep(60*3)\n",
    "        with open(\"inference_llama_first_80b.json\", \"w\") as f:\n",
    "            json.dump(first_llama_inference, f, indent=4)\n",
    "            \n",
    "        first_llama_inference.append({\"image\": r[\"image\"], \"description\": invoke_with_rate_limit_per_minute(r[\"description_llava\"], r[\"description_kosmos\"], entities)})\n",
    "        \n",
    "with open(\"inference_llama_first_80b.json\", \"w\") as f:\n",
    "        json.dump(first_llama_inference, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de393a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "44d842e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "611"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checked_images = pd.DataFrame(first_llama_inference).image.unique().tolist()\n",
    "len(checked_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "562c7b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"inference_llama_first_80b.json\", \"w\") as f:\n",
    "        json.dump(first_llama_inference, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5a531a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uav_img_cap",
   "language": "python",
   "name": "uav_img_cap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}