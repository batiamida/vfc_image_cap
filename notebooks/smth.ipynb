{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1126f302-5911-40e9-9a02-32934c2fb91a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install langchain langchain-community huggingface_hub roboflow pillow transformers requests tqdm pandas\n",
    "\n",
    "\n",
    "\n",
    "# !pip install -q diffusers transformers accelerate peft\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8483a631-8fc2-4e38-b276-a0efd135925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_llama2_general = \"\"\"\n",
    "You are an advanced language model tasked with merging detailed captions generated by two multimodal systems. Your goal is to create a unified, highly detailed, and factually accurate description of the image.\n",
    "\n",
    "### Instructions:\n",
    "- Combine the details from both captions, ensuring all critical elements (objects, actions, spatial relationships, and contextual nuances) are represented.\n",
    "- Maintain a coherent and consistent tone throughout the output.\n",
    "- Avoid contradictions or redundant details; instead, refine and enhance the combined information for clarity.\n",
    "- Incorporate additional entities provided, enriching the description with fine-grained visual and contextual details.\n",
    "- The resulting caption should serve as the most comprehensive and accurate summary of the image.\n",
    "\n",
    "### Input Captions:\n",
    "1. Caption 1: {caption1}\n",
    "2. Caption 2: {caption2}\n",
    "\n",
    "### Additional Entities:\n",
    "- {entities}\n",
    "\n",
    "### Output:\n",
    "Write a single, unified caption that captures the full essence of the image, accurately representing all provided details and entities.\n",
    "\"\"\"\n",
    "\n",
    "prompt_template_llama2_object_detection = \"\"\"\n",
    "You are an advanced language model tasked with merging detailed captions generated by two multimodal systems. Your goal is to create a unified, highly detailed, and factually accurate description of the image.\n",
    "Below is a detailed description of an image. Your task is to extract and list every object mentioned in the description that can be reliably detected by an object detection model. Please follow these instructions carefully:\n",
    "\n",
    "### Instructions:\n",
    "1. Identify each object that is clearly described and can be recognized by typical object detection systems.\n",
    "2. Be very cautious with numerical details attached to the objects.\n",
    "3. Do not combine or generalize objects if different numbers are specified. Each unique instance should be explicitly listed.\n",
    "\n",
    "### Detailed Description:\n",
    "{description}\n",
    "\n",
    "### Output:\n",
    "List all identifiable objects from the detailed description. Return it in json with key named results\n",
    "Given the description as an input, provide a response in this exact JSON schema:\\n\\n\"\n",
    "    {\n",
    "    \"results\": \"<list of objects>\"\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "final_caption_prompt_template = \"\"\"\n",
    "You are an advanced language model tasked with refining a detailed image caption. Your goal is to ensure that the caption is both highly detailed and free of hallucinations by cross-checking it against detection probabilities from an object detection model.\n",
    "\n",
    "### Instructions:\n",
    "1. Carefully analyze the provided detailed description of the image.\n",
    "2. For each object in the description:\n",
    "   - Cross-check its presence using the detection probabilities from the object detection model.\n",
    "   - If the probability of detection for an object is high (e.g., above 0.1), retain the object in the caption.\n",
    "   - If the probability of detection for an object is low (e.g., below 0.1), treat it as a potential hallucination and decide whether to remove it from the caption.\n",
    "   - If the object is not present in the detection probabilities, leave it unchanged in the caption.\n",
    "3. Ensure that the revised caption maintains logical flow, coherence, and a consistent tone after removing hallucinated objects.\n",
    "4. Do not add any new objects or make assumptions beyond the provided information.\n",
    "\n",
    "### Inputs:\n",
    "- Detailed Description: {description}\n",
    "- Detection Probabilities: {object_probabilities}\n",
    "\n",
    "### Output:\n",
    "Write a revised, highly detailed caption where all described objects are verified as detected and potential hallucinations are removed.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcfafb6d-5c95-4c2d-9ae4-16fa795a66eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "root_dir = os.getcwd()\n",
    "path_to_kosmos = os.path.join(root_dir, \"inference_results_kosmos_2.json\")\n",
    "path_to_llava = os.path.join(root_dir, \"inference_results_llava_first_step.json\")\n",
    "\n",
    "kosmos_res = pd.read_json(path_to_kosmos)\n",
    "llava_res = pd.read_json(path_to_llava)\n",
    "\n",
    "first_step_df = kosmos_res.merge(llava_res, on=\"image\", suffixes=(\"_kosmos\", \"_llava\")).drop_duplicates(subset=\"image\", keep=\"last\")\n",
    "# with open(path_to_file, \"rb\") as f:\n",
    "#     kosmos_inference_res = json.loads(f.read())\n",
    "# pd.read_json(kosmos_inference_res)\n",
    "inputs = first_step_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d2cdcdf-8e17-4f77-9ec2-238f959bf1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "# from langchain_community.chat_models import ChatOllama\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from transformers import pipeline\n",
    "import timeit\n",
    "\n",
    "\n",
    "local_llm = \"llama2:13b\"\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "prompt_general = PromptTemplate(template=prompt_template_llama2_general, input_variables=[\"caption1\", \"caption2\", \"entities\"])\n",
    "prompt_object_detection = PromptTemplate(template=prompt_template_llama2_object_detection, input_variables=[\"description\"])\n",
    "prompt_final = PromptTemplate(template=final_caption_prompt_template, input_variables=[\"description\", \"object_probabilities\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61de5b9e-84ea-44cd-8a5d-7e396d33acb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First step llama2 generation: 1035it [1:32:38,  5.38s/it]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import timeit\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# # Initialize the tokenizer and model, explicitly transferring them to GPU\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n",
    "\n",
    "# # Set the model to evaluation mode\n",
    "# model.eval()\n",
    "\n",
    "# Initialize results list\n",
    "first_step_responses = []\n",
    "\n",
    "# Start timer\n",
    "start = timeit.default_timer()\n",
    "\n",
    "for i, inputs in tqdm(first_step_df.iterrows(), desc=\"First step llama2 generation\"):\n",
    "    # Preprocess input: tokenize the prompt\n",
    "    inputs_text = dict(\n",
    "        caption1=inputs[\"description_llava\"].split(\"ASSISTANT:\")[1],\n",
    "        caption2=inputs[\"description_kosmos\"],\n",
    "        entities=\", \".join([entities[0] for entities in inputs[\"entities\"]])\n",
    "    )\n",
    "\n",
    "    # Tokenize the inputs and move to GPU\n",
    "    inputs_enc = tokenizer(inputs_text[\"caption1\"], inputs_text[\"caption2\"], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Run inference\n",
    "    with torch.no_grad():  # Disable gradients for inference\n",
    "        # Generate response\n",
    "        output = model.generate(\n",
    "            inputs_enc[\"input_ids\"],\n",
    "            temperature=0.2,\n",
    "            max_new_tokens=100,\n",
    "            top_k=50,\n",
    "            top_p=0.95\n",
    "        )\n",
    "\n",
    "    # Decode the generated text and append response\n",
    "    response_first = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    first_step_responses.append({\n",
    "        \"image\": inputs[\"image\"],\n",
    "        \"description_llama_first\": response_first\n",
    "    })\n",
    "\n",
    "    # Save results in batches\n",
    "    if len(first_step_responses) % 100 == 0:\n",
    "        with open(\"inference_llama_first.json\", \"w\") as f:\n",
    "            json.dump(first_step_responses, f, indent=4)\n",
    "\n",
    "# Save final results\n",
    "with open(\"inference_llama_first.json\", \"w\") as f:\n",
    "    json.dump(first_step_responses, f, indent=4)\n",
    "\n",
    "# End timer\n",
    "end = timeit.default_timer()\n",
    "\n",
    "# Save inference time\n",
    "inf_time = {}\n",
    "with open(\"inference_time.json\", \"w\") as f:\n",
    "    inf_time[\"inference_llama_first_time\"] = end - start\n",
    "    json.dump(inf_time, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed3bbd7-d484-4a09-a992-74a7d12d2e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "start = timeit.default_timer()\n",
    "\n",
    "second_step_responses = []\n",
    "\n",
    "for response_first in tqdm(first_step_responses, desc=\"Second step llama2 generation\"):\n",
    "    # Preprocess input: tokenize the description\n",
    "    formatted_description = prompt_object_detection.format(description=response_first[\"description_llama_first\"])\n",
    "    inputs_enc = tokenizer(formatted_description, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Run inference with torch.no_grad() to save memory\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            inputs_enc[\"input_ids\"],\n",
    "            max_new_tokens=100,  # Control the number of tokens generated\n",
    "            temperature=0.5,\n",
    "            top_k=50,\n",
    "            top_p=0.95\n",
    "        )\n",
    "\n",
    "    # Decode the generated response\n",
    "    response_second = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Append the results with image info\n",
    "    second_step_responses.append({\n",
    "        \"image\": response_first[\"image\"],\n",
    "        \"objects_llama_second\": response_second  # Use the decoded text as the result\n",
    "    })\n",
    "\n",
    "    # Save results in batches\n",
    "    if len(second_step_responses) % 100 == 0:\n",
    "        with open(\"inference_llama_second.json\", \"w\") as f:\n",
    "            json.dump(second_step_responses, f, indent=4)\n",
    "\n",
    "# Final result saving\n",
    "with open(\"inference_llama_second.json\", \"w\") as f:\n",
    "    json.dump(second_step_responses, f, indent=4)\n",
    "\n",
    "# End timer\n",
    "end = timeit.default_timer()\n",
    "\n",
    "# Save inference time\n",
    "inf_time = {}\n",
    "with open(\"inference_time.json\", \"w\") as f:\n",
    "    inf_time[\"inference_llama_second_time\"] = end - start\n",
    "    json.dump(inf_time, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
